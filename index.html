<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


  <title>Zero-shot Emotion Transfer for Cross-lingual Speech Synthesis</title>
  <meta name="generator" content="Jekyll v3.9.0">
  <meta property="og:title" content="TODO: title">
  <meta property="og:locale" content="en_US">

  <meta name="twitter:card" content="summary">


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="style.css">
</head>

<body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
  <section class="page-header">



  </section>

  <section class="main-content">
    <h1 id="">
      <center>Zero-shot Emotion Transfer for Cross-lingual Speech Synthesis</center>
    </h1>


<!--     <h2>0. Contents</h2>
    <ol>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#transfer">Examples of target speaker</a></li>
      <li><a href="#prediction">Synthetic speech of benchmark systems</a></li>
      <li><a href="#control">Synthetic speech of ablation studies</a></li>
    </ol> -->

    <br><br>
    <h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
    <p> Zero-shot emotion transfer in cross-lingual speech synthesis aims to transfer emotion from an arbitrary speech reference 
      in the source language to the synthetic speech in the target language. Building such a system faces challenges of unnatural
      foreign accents and difficulty in modeling the shared emotional expressions of different languages. This paper based on 
      DelightfulTTS addresses these challenges by leveraging different modules to model the language-specific prosody features 
      and language-shared emotional expressions separately. Specifically, the language-specific speech prosody is learned by a 
      non-autoregressive predictive coding (NPC) to improve the naturalness of the synthetic cross-lingual speech. The shared emotional 
      expression between different languages is extracted from a pre-trained self-supervised model Hubert with strong generalization
      capabilities. We furthermore use hierarchical emotion modeling to capture more comprehensive emotions across different languages. 
      Experimental results demonstrate the proposed framework's effectiveness in synthesizing bi-lingual emotional speech for the 
      monolingual target speaker without emotional training data.
    </p>
    <center><img src='fig/pin.png'></center>
    <div align="center"><b>Fig.1</b> The architecture of (a) overview and (b) the hierarchical emotion encoder.</div>

    <br><br>

    <h2>2. Demos -- <a name="Comparison"></a></h2>
    <ul>
      <li>M3: .</li>
      <li>SCLN-GST: .</li>
      <li>PROPOSED: .</li>
      <li>PROPOSED-NPC: .</li>
      <li>PROPOSED-Deep emo: .</li>
    </ul>
   

    <table>
      <tbody id="tbody">
      </tbody>
    </table> 

    <footer class="site-footer">

      <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com/">GitHub
          Pages</a>.</span>
    </footer>
  </section>
</body>

</html>

<script type="" text/javascript>
  window.onload = function () {
    let scenes = ["0000", "21"]
    let speakers = ["db_1", "M2VoC_male", "SSB1956", "SSB0273"]
    // let speakers = ["db_1", "M2VoC_male"]
    // let genders = ["female", "male"]
    let genders = ["female", "male", "aishell female", "aishell male"]
    let models = ["Bottomline", "Baseline", "Topline", "DualVC-nonstreaming", "DualVC-streaming"]
    let all_samples = [["1.wav", "4.wav", "biaobei_8315010.wav", "TenXiaozhi_007_000472.wav"],["1.wav", "4.wav", "biaobei_8315010.wav", "TenXiaozhi_007_000472.wav"]]
    let sample_data = `
        <tr>
          <td style="text-align: center; width: 150px;" rowspan=2><strong>Scenario<strong></td>
          <td style="text-align: center; width: 150px;" rowspan=2><strong>Target Speaker<strong></td>
          <td style="text-align: center; width: 150px;" rowspan=2><strong>Source speech<strong></td>
          <td style="text-align: center; width: 150px;" colspan=5><strong>Method<strong></td>
        </tr>
        <tr>
        `
    for (const id in models) {
      model = models[id]
      sample_data += '<td style="text-align: center; width: 150px;" rowspan=1><strong>' + model + '<strong></td>'
    }
    sample_data += "</tr>"
    console.log(sample_data)
    
    for (let x in scenes) {
      let scene = scenes[x]
      let scene_data = ""
      scene_data += '<tr>'
      scene_data += '<td style="text-align: center; width: 150px;" rowspan=' + 16 + '><strong>' + scene + ' Source' + '<strong></td>'
      let samples = all_samples[x]
      console.log(scene, samples)
      for (let y in speakers) {
        speaker = speakers[y]
        gender = genders[y]
        scene_data += '<td style="text-align: center; width: 150px;" rowspan=4>' + gender + '<audio style="width: 150px;"controls="" src="raw/samples/speakers/' + speaker + '.wav"></td>'
        for (let z in samples) {
          if (z != 0) {
            scene_data += '<tr>'
          }
          let sample = samples[z]
          scene_data += '<td style="text-align: center"><audio style="width: 150px;" controls="" src="raw/samples/' + scene + '/source/' + sample + '"></audio></td>'
          for (let w in models) {
            let model = models[w]
            console.log(speaker + model)
            if ((speaker == "SSB1956" || speaker == "SSB0273") && (model != "DualVC-streaming" && model != "DualVC-nonstreaming")) {
              console.log("!!!")
              scene_data += '<td style="text-align: center"><p>N/A</p></td>'
            } else {
              scene_data += '<td style="text-align: center"><audio style="width: 150px;" controls="" src="raw/samples/' + scene + '/' + model + '/' + speaker + '/' + sample + '"></audio></td>'
            }
          }
          scene_data += '</tr>'
        }
      }
      sample_data += scene_data
    }
    document.getElementById('tbody').innerHTML = sample_data
  }
</script>
